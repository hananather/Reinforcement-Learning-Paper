\section{Introduction}
\subsection{What is Reinforcement Learning?}
Reinforcement learning is a subfield of artificial intelligence that centers around an agent learning to make optimal decisions by interacting with its environment, receiving feedback in the form of rewards or penalties, and adjusting its behavior accordingly to maximize cumulative rewards over time \cite{RL}. In this process, the agent gathers information through actions and interactions with the environment, with its performance evaluated by the rewards it receives, which it ultimately aims to maximize.

Upon taking an action, the agent obtains two pieces of information: its present state within the environment and a real-valued reward. The agent's goal is to maximize its reward, thereby determining the most effective sequence of actions, or policy, to accomplish its objective. However, the information it acquires from the environment pertains solely to the immediate reward associated with the actions just executed.
\begin{figure}
    \centering
    \tikzstyle{block} = [rectangle, draw, fill=blue!20,
    text width=8em, text centered, rounded corners, minimum height=4em]

    \tikzstyle{line} = [draw, -latex, color=blue]

    \begin{tikzpicture}[node distance = 6em, auto, thick]
        \node [block] (Agent) {\textbf{Agent}};
        \node [block, below of=Agent] (Environment) {\textbf{Environment}};
        
        \path [line] (Agent.0) --++ (4em,0em) |- node [near start]{Action $a_t$} (Environment.0);
        \path [line] (Environment.190) --++ (-6em,0em) |- node [near start] {New state  $s_{t+1}$} (Agent.170);
        \path [line] (Environment.170) --++ (-4.25em,0em) |- node [near start, right] {Reward $r_{t+1}$} (Agent.190);
    \end{tikzpicture}
    \caption{The MDP Framework: As illustrated in the figure, the agent, which represents an AI algorithm, and the environment, an abstract entity delivering uncertain outcomes, engage in a time-sequenced, iterative interaction.}
    \label{fig:my_label}
\end{figure}


\subsection{Supervised, Unsupervised, \& Reinforcement Learning}
In contrast to supervised learning, where the learner trains passively using labeled data, reinforcement learning involves active engagement with the environment, and the policy chosen defines the distribution over observations rather than relying on a fixed distribution \cite{jordan2015}. Minor alterations to the policy can significantly impact the rewards obtained, highlighting the dynamic nature of reinforcement learning. Additionally, the environment may not remain constant and could change due to the actions selected by the agent. Reinforcement learning also differs from supervised learning in that training and testing phases are intertwined, rather than being separate stages of the process. 
Unlike other branches of machine learning, such as supervised and unsupervised learning, reinforcement learning embodies a more ambitious approach. It not only extracts patterns and properties from the given data, but also discerns suitable behaviors and decisions to progress towards the desired optimization objective. One could argue that supervised and unsupervised learning primarily concentrate on "minimization," as they strive to minimize the fitting error of a model to the provided data. Conversely, reinforcement learning focuses on "maximization," seeking to identify the most appropriate decisions that contribute to the maximization of a well-defined objective.

In reinforcement learning, the agent confronts a dilemma between exploring unfamiliar states and actions to gather more information about the environment and rewards, and exploiting the existing information to optimize its reward. This challenge, known as the exploration versus exploitation trade-off, is a fundamental aspect of the reinforcement learning process.

\section{Basic Components of RL}
\subsection{Markov Decision Process}
A Markov decision process (MDP) is a mathematical framework used in reinforcement learning to model decision-making in situations where an agent interacts with an environment that has probabilistic state transitions and rewards \cite{Markov}. 

The motivation for using MDPs in reinforcement learning is to provide a structured representation of the problem, facilitating the development of algorithms that enable agents to find optimal policies for maximizing cumulative rewards over time \cite{RL2}.
\begin{definition}[Markov Decision Process]
    A \vocab{Markov decision process} (MDP) is defined by 4-tuple ($\cS$, $\SA$, $\SP_a$, $\SR$):
    \begin{enumerate}
        \item a set of states $\cS$, possibly infinite
        \item a set of actions $\SA$, possibly infinite
        \item  transition probability $\SP_a(s',s) = \PP [S_{t+1} = s'| S_{t} = ,A_{t} = a]$
        \item a reward $R_{t+1} \in \SR \subseteq \RR$, and corresponding probability distribution $ \SR_a(s',s) = \PP[r' | s,a]$ over rewards
    \end{enumerate}
\end{definition}

In a \textit{finite} MDP, random variables $R_t$ and $S_t$ possess well-defined discrete probability distributions, which depend solely on the preceding state and action. For specific values $r \in \SR$ and $s' \in \cS$, there exists a probability of these values occurring at time $t$, given particular values of the preceding state and action. The transition probabilities from one time step to the next can be represented as a function $\SP: \cS \times \SR \times 
\cS \times \SA \to [0,1]$, which is an ordinary deterministic function\cite{RL}:

\begin{equation}
    \SP(s', r \mid s, a) = \mathbb{P}\{S_{t+1} = s', R_{t+1} = r \mid S_t = s, A_t = a\}
\end{equation}
Markov Decision Processes (MDPs) have been used to model a wide range of decision-making problems across various domains, some read-world examples include:
\begin{itemize}
    \item Robotics: In robotics, MDPs are employed for navigation and path planning tasks \cite{thrun2005}
    \item Finance: MDPs have been used to model and solve portfolio optimization problems \cite{buehler2019}
    \item Healthcare: MDPs have been employed in healthcare for personalized treatment recommendation system 
    \cite{komorowski2018}
\end{itemize}

\subsection{Policy and Return}
In reinforcement learning algorithms, assessing the value of an agent being in a particular state is crucial. 
Almost all reinforcement learning algorithms involve estimates of how good is it for the agent to be in a given state \cite{RL}.
However, defining ``how good" a state is can be ambiguous. In RL, the value of a state is determined by anticipated future rewards, or more precisely, in terms of expected rewards.

Of course the rewards the agent can expect to receive in the future depends on the actions it will take. Accordingly, the values of states (i.e., the expected reward agent will receive from that state on wards) are defined with respect to a \vocab{policy}.

In reinforcement learning, a policy is a strategy or set of rules that guides an agent's decision-making process, determining which action to take in a given state to maximize cumulative rewards over time. Policies are just functions that take as input a state, $S_t$, and output a probability distribution of actions.
For a given state $s \in S$ and action $a \in A$, the policy is defined as \cite{mohri2018}:
\begin{definition}[Policy]
    A policy is a mapping $\pi: S \to \Delta(A),$ where $\Delta(A)$ is the set of probability distributions over the action space $A$. A policy is deterministic if for any $s \in S$, there exists a unique $a \in A$ such that $\pi(a|s) = 1$. In this case, we can identify $\pi: S \to A$.
    \footnote{The given definition pertains to a stationary policy, as the action distribution does not rely on time. Generally, a non-stationary policy can be characterized as a sequence of mappings $\pi_t : S \rightarrow \Delta(A)$, indexed by $t$. Notably, in scenarios with a finite horizon, employing a non-stationary policy is often essential for reward optimization}
\end{definition}
The objective in RL is learn a policy, more precisely, the agent's objective is to find a policy that \textit{maximizes its expected (reward) return}. The discounted return is defined as \cite{RL}:
\begin{equation}
    G_t = R_{t+1} + \gamma R_{t+2 } + \gamma^2 R_{t+3 } + \cdots = \gamma^k \sum_{k=0}^{\infty} R_{t+k+1}
\end{equation}
Where $\gamma \in [0,1]$ is the \textit{discount-factor} \footnote{ The discount factor $\gamma$ serves to model scenarios in which a future reward is considered less desirable than an immediate reward of the same magnitude.}
The return is a scalar-value that essentially summarizes a (possibly) an infinite sequence of immediate rewards. 
An important fact to note is that even though this is an infinite sum, if $\gamma < 1$, $G_t$ will be finite. If we set the value of $\gamma$ to $0$, the agent will only consider immediate reward, and future rewards will be of no consequence. Conversely, if $\gamma = 1$, then the agent values all future rewards equal to the immediate reward \cite{RL}.


\subsection{Value and State-Value Functions}

Now we can define the notion of ``value of a state" formally via \vocab{value functions}. Value functions in RL are tools for estimating the long-term `value' or desirability of being in a given state. They allow the agent to access how good it is to be in a certain state, based on the expected cumulative reward obtained from that point onwards. There are two types of value functions commonly used in reinforcement learning: $V^{\pi}(s)$ and $Q^{\pi}(s,a)$, we define them formally below \cite{RL}, \cite{mohri2018}:
\begin{definition}[Value function]
    The value $V^{\pi}(s)$ for a fixed policy $\pi$ at a given state $s \in S$ represents the expected reward obtained when initiating at state $s$ and adhering to policy $\pi:$
    \begin{itemize}
        \item $V^{\pi}(s) = \underset{\pi, p}{\EE} [G_t \mid S_t = s]$for all $s \in S$ \cite{RL}
        \item finite horizon: $V^{\pi}(s) = \underset{a_t \sim \pi(s_t)}{\EE} \left[ \sum_{t=0}^T r(s_t,a_t) \mid S_t =s
        \right]$ \cite{mohri2018}
        \item infinite discounted horizon: $V^{\pi}(S_t = s) = \underset{a_t \sim \pi(s_t)}{\EE} \left[ \sum_{t=0}^{+ \infty} \gamma^t r(s_t,a_t) \mid S_t =s
        \right]$ \cite{mohri2018}
    \end{itemize}
    \footnote{Technically the notation should be: $\EE_{a_t \sim \pi(s_t)} \EE_{s_t}$ since
    we are not just taking the expectation over the random selection of an action $a_t$ according to the distribution $\pi(s_t)$, but we are also taking the expectation over the states $s_t$ reached and the corresponding reward values $r(s_t, a_t)$. However, in RL literature/textbooks the randomization with respect to the next state and reward function doesn't seem to be  explicitly mentioned (to simplify notation perhaps)} 
\end{definition}
An intuitive way of understanding Value Function is that it tells us how much ``accumulated future reward'' we expect to obtain from a given state .

Starting from a state $s \in S$, to maximize reward, an agent naturally seeks a policy $\pi$ with the largest value $V_{\pi}(s)$. A remarkable result for finite MDPs in the finite horizon case, there exists an \textit{optimal policy} for all starting states $s \in S$. Formally, a policy $\pi^*$ is optimal if for any policy $\pi$ and any state $s \in S$, $V^{\pi^*}(s) \geq  V^{\pi}(s)$. 
We can prove that this optimal policy $\pi^*$ is deterministic \cite{mohri2018}, and this proof can be found in Mohri  et al. (2018). 
\\\
\\
\textbf{Action value function} ($Q^{\pi}$): Represents the expected cumulative reward an agent can obtain by taking an action a in a given state s and following a certain policy $\pi$ thereafter. 

\begin{definition}[State-action value function]
    The state-action value function $Q$ associated to a policy $\pi$ is defined for all $(s,a) \in S \times A$ as the expected return for taking action $a \in A$ at state $s \in S$ and then following policy $\pi$:
    \begin{align*}
    Q^{\pi}(s,a) &=  \underset{\pi}{\EE} [G_t \mid S_t =s, A_t = a]  = \underset{\pi}{\EE}
    \left [
    \sum_{k =0}^{+ \infty} \gamma^t R_{t+k+1} \mid S_t = s, A_t = a \right ]
    \end{align*}
\end{definition}
The motivation behind using value functions in RL is to guide the agents decision-making process. By estimating the values of states or state-action pairs, the agent can choose actions that lead to states with higher-long term rewards. We will see in next section that values serve as the basis for various RL algorithms, such as $Q-$learning and SARSA.

Value functions $V^{\pi}$ and $Q^{\pi}$ can be estimated through experience. For instance, if an agent follows policy $\pi$ and maintains an average for each state encountered, based on the actual returns following that state, these averages will converge to $V^{\pi}(s)$ for all $s \in S$ as the number of times the state is encountered approaches infinity.
By keeping separate averages for each action taken in each state, these averages will converge to the action-values $Q^{\pi}(s,a)$. Such estimation methods are referred to as \textbf{Monte Carlo methods} because they involve averaging over numerous samples of actual returns.
\subsection{The Bellman Equations}
The Bellman equation is a fundamental equation in reinforcement learning and in dynamic programming\cite{bellman1958}. There are two forms of Bellman equation: (1) Bellman Expectation Equation and (2) Bellman Optimally Equation. 
Both equations expresses the relationship between the value of a state and values of successor states. 
\note{In simple terms, the Bellman Equation decomposes the value function into immediate reward from taking an action in the current state plus the discounted expected value of future states, while taking into account all future possible actions.}
\vocab{Bellman Expectation Equation} is used to calculate the used to compute the value function $V^\pi(s)$ for a given policy $\pi$.
\begin{align*}
    V^{\pi}(s) &= \EE_{\pi}[G_t \mid S_t =s] \\
    &= \EE_{\pi}[R_{t+1} + \gamma G_{t+1} \mid S_t =s] \\
    &= \sum_{a \in A} \pi(a\mid s) \sum_{s'} \sum_{r} \PP(s',r \mid s,a) [r + \gamma \term{$\EE_{\pi} (G_{t+1} \mid S_{t+1} = s')$} ] \\
    &=\sum_{a \in A} \pi(a\mid s) \sum_{s'} \sum_{r} \PP(s',r \mid s,a) [r + \gamma \term{$V^{\pi}(s')$} ], \quad \forall s \in S
\end{align*}
Think of looking ahead from a state to its successor state(s). 
The Bellman equation tells us that the value of any state must be equal to the expected reward received form being being in that state plus the (discounted) value of the expected next state.
Another more compact way to express the Bellman equation commonly found in the literature is: 
\begin{equation}
    V^{\pi}(s) = \sum_{a \in \SA(s)} \pi(a|s) \cdot \sum_{s'} \SP_{s,s'}^{a} [r(s,a) + \gamma V^{\pi}(s')]
\end{equation}
The equation is allowing us to compute the value of state $s$ if we follow an arbitrary policy  $\pi$. In the equation above $r(s,a)$ is the immediate reward. 
The equation states that the value of a given state, $V^{\pi}(s)$ under $\pi$, is equal to the immediate reward $r(s,a)$ plus discounted value of the next state $\gamma V^{\pi}(s')$. Recall that $V^{\pi}(s')$ is also an expectation, and therefore, it is considering all possible actions and transitions to the next state , weighted by their probabilities $\SP_{s,s'}^{a} = \PP(s' \mid s,a)$.
\\
\vocab{Bellman Equation for Action-Value Function} can be expressed in the same way:
\begin{align}
    Q^{\pi}(s,a) &= \SP_{s,s'}^{a} [ r(s,a) + \gamma \overbrace{\sum_{a'} \pi(a'|s') Q^{\pi}(s',a')}^{V^{\pi}(s)}] \\
    &= \SP_{s,s'}^{a} [ r(s,a) + \gamma V^{\pi}(s)]
\end{align}
Since, 
$$
V^{\pi}(s) = \sum_{a \in \SA(s)} \pi(a \mid s) Q^{\pi}(s,a)
$$
\subsection{Finding Optimal Policies}
Value functions allow us to create a hierarchical structure among policies, where a policy $\pi$ is deemed superior or equal to another policy $\pi'$ if it yields an expected return that is greater than or equal to that of $\pi'$ across all states\cite{RL2}. Thus, $\pi \geq \pi'$ if and only if the condition $V_\pi(s) \geq V_{\pi'}(s)$ holds true for every state $s \in S$ \cite{mohri2018}. For finite MDPs, there is always at least one policy that is better than or equal to all other policies \cite{RL}. This is what we call the \textit{optimal policy}. Although there might be more than one, we denote all the optimal policies by $\pi^*$. The optimal policies share the same state-value function and the action-value functions which we denote as $V^{*}$ and $Q^{*}$\cite{RL2}.
\begin{align*}
    V^{*}(s) = \underset{\pi}{\text{max }} V^{\pi}(s), \quad  \forall s \in S, a \in A \\ 
    Q^{*}(s) = \underset{\pi}{\text{max }} Q^{\pi}(s), \quad  \forall s \in S, \forall a \in A
\end{align*}
A useful fact which will be leveraging is that we can express $Q^*$ in terms of $V^*$:
\begin{align*}
    Q^*(s,a) &= \EE [R_{t+1} + \gamma V^*(S_{t+1}) \mid S_t = s, A_t =a ] \\ 
    &= \EE [R_{t+1}]  + \gamma \sum_{ s' \in \cS} \PP[S_{t+1} =s'|S_t = s, A_t = a] V^*(S_{t+1})   
\end{align*}
Note that $R_{t+1}$ is a random varible since we don't know what future state $s'$ we will end up in, therefore, 
we are taking the expectation $\EE [R_{t+1}]$ over the dynamics of transition probabilities of the next state $s'$ we reach. 
\\
\vocab{Bellman Optimality Equation:} This equation is expresses that ``the value of  state under \textit{optimal policy} must be equal to the expected for the best action from that that state"\cite{RL}, and
is used to compute the optimal value function which is usually denoted as $V^*(s)$ in literature.
\begin{align*}
    V^*(s) = \underset{a \in A}{\text{max }} Q^{\pi}(s), \quad  \forall s \in S 
\end{align*}
The Bellman optimally equation for $Q^*(s,a)$ is 
\begin{align*}
    Q^{*}(s,a) &= \EE \left [R_{t+1}
    + \gamma \underset{a}{\text{max}}Q^*(S_{t+1},a')  \mid S_{t} =s, A_t = a \right] \\
    &= \sum_{s',r}\PP(s',r \mid s,a)
    \left [r + \gamma \underset{a}{\text{max}}Q^*(S_{t+1},a') 
    \right]
\end{align*}


\begin{itemize}
    \item $V^*(s)$: once we have $V^*$ it is relatively easy to determine the optimal policy (we are working backwards here, we started off defining the $V$ function for specific policies, but now we are using the $V^*$ to find the policy). The beauty of $V^*$ is that it makes the greedy policy the optimal policy in the long-term sense because $V^*$ is taking into account the reward consequences all possible future behaviour. By means of $V^*$, the optimal expected long-term return is turned into a quantity that is locally and immediately available for each state.
    
    \item $Q^*(s,a)$: Having $Q^*$ makes choosing optimal actions easier, with $Q^*$ the agent does even have to do one-step ahead search. Given any state $s \in S$, it can simply find any action that maximizes $Q^*(s,a)$. The action value function caches the result of one-step-ahead searches. It provides the expected long-term return as a value that is locally and and immediately available for each action-pair.
\end{itemize}





\subsection{GPI}
When we don't have a model, we don't have access to the environment's dynamics such as transition probabilities ($\PP(S_{t+1} \mid S_t , A_t)$) or rewards functions $r(s_{t+1}, a_t, s_t)$ (reward for taking action $a_t$ in state $s_t$ and transitioning state $s_{t+1}$). Consequently, we cannot predict the next state and the immediate reward you will receive when taking a specific action in a given state. 

In \vocab{Generalized Policy iteration (GPI)} one maintains both an approximate policy and an approximate value function. The value function is continually altered to obtain a more accurate approximation of the value function, and the policy is repeatedly improved with respect to the current valuye function. 
 

On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off policy methods evaluate or improve a policy different from that used to generate the data. 
In on-policy control methods rthe policy is generally soft, meaning that $\pi(a \mid s) > 0$ for all $s \in S$ and $a \in A(s)$, but gradually shifted closer and closer to a deterministic optimal policy. 


